\documentclass[12pt]{article}
\usepackage{Diploma}

%\addbibresource{my_bib}

\begin{document}
	
	{
		\renewcommand{\baselinestretch}{1}
		\thispagestyle{empty}
		\begin{center}
			\sc
			Министерство науки и высшего образования Российской Федерации\\
			Московский физико-технический институт\\
			{\rm(национальный исследовательский университет)}\\
			Физтех-школа прикладной математики и информатики\\
			Магистерская программа\\
			<<Методы и технологии искусственного интеллекта>>\\[35mm]
			\rm\large
			Сотников Антон Дмитриевич\\[10mm]
			\bf\Large
			Байесовский выбор архитектуры нейросетевой модели\\[10mm]
			\rm\normalsize
			03.04.01 --- Прикладные математика и физика\\[10mm]
			\sc
			Магистерская диссертация\\[30mm]
		\end{center}
		\hfill\parbox{80mm}{
			\begin{flushleft}
				\bf
				Научный руководитель:\\
				\rm
				к ф.-м. н\\
				Бахтеев Олег Юрьевич\\[5cm]
			\end{flushleft}
		}
		\begin{center}
			Москва\\
			2022 г.
		\end{center}
	}
	
	\newpage
	\tableofcontents
	
	\newpage
	\begin{abstract}
		В работе исследуется задача выбора структуры модели нейронной сети. Предлагается метод, вычисляющий апостериорное совместное распределение структуры и параметров модели с помощью байесовского вывода. Вводятся априорные распределения на параметры и структуру модели. В силу практической невычислимости апостериорное распределение предлагается оценивать с помощью оптимизации вариационной нижней оценки. Анализируется робастность метода относительно внесения шума в параметры структуры. Для проведения вычислительного эксперимента используются выборки CIFAR-10 и Fashion-MNIST.
		
		
	\end{abstract}
	
	\newpage
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Введение}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	Нейронные сети показывают высокое качество в решении многих задач, таких как детектирование объетов на изображении, построение языковых моделей и т.д (тут цитирование). Однако формирование архитектуры, способной решить задачу с высоким качеством, требует большой экспертизы, что вносит большое ограничение. Недавно появился подход, называемый поиском архитектуры нейросетевой модели (ПАНМ). Он заключается в том, что автоматически генерирует архитектуру модели для заданных задачи и набора данных. Методы ПАНМ продемонстрировали возможность решать задачи лучше, чем созданные вручную архитектуры (тут цитирование).
	
	Хоть методы ПАНМ решают задачи с высокой точностью, но они уязвимы к состязательным атакам (тут цитирование). Такие атаки потенциально могут быть очень опасны \cite{DBLP:journals/corr/PapernotMGJCS16}.В (тут цитирование) описано, что при внесении небольшого шума объект на изображении начаинает классифицироваться неверно. 
	
	Одним из недостатков методов ПАНМ является их уязвимость к состязательным атакам (тут цитирование)
	
	Многие известные архитектуры показывают высокие результаты на общедоступных наборах данных, но когда дело доходит до специфичного набора данных, то показывают плохое качество. Поэтому актуальна задача nas. Она помогает для конкретного набора данных найти оптимальную с точки зрения метрики качества структуру нейронной сети, которая превосходит по качеству разработанные вручную аналоги.
	
	Также актуальна задача повышения робастности относительно состязательных атакю бла бла
	
	
	\textbf{Цели и задачи исследования.} Основной целью исследования является построение робастного к состязательным атакам метода поиска архитектуры нейросетевой модели с применением байесовского вывода. Для реализации этой цели поставлены следующие задачи:
	\begin{itemize}
		\item изучить существующие методы решения задачи поиска архитектуры нейросетевой модели;
		\item изучить методы применения состязательных атак и внесения шума в структуру и параметры метода ПАНМ (поиска архитектуры нейросетевой модели) \textcolor{red}{мб ввести сокращение ПАНМ? оно выглядит коряво конечно, но так читать проще будет кажется};
		\item провести вариационный вывод оценки апостериорного распределения параметров и структуры;
		\item предложить теоретические интерпретацию и обоснование предлагаемого метода;
		\item реализовать метод в виде программного кода на языке Python;
		\item провести вычислительный эксперимент и получения значения метрик качества.
	\end{itemize}
	
	\textbf{Научная новизна.} Предложен метод построения робастной к состязательным атакам модели глубокого обучения, основанный на градиентном подходе поиска архитектуры. \textcolor{red}{Представлено теоретическое обоснование (доказательства) описанного метода.}
	
	\textbf{Методы исследования.} Для оценки совместного апостериорного распределения параметров и структуры используется вариационная нижняя оценка обоснованности. Обучение модели проводится градиентными методами.
	
	\textbf{Практическая ценность.} Предложенный метод предназначен для построения моделей, основанных на нейронных сетях, и их применения в задачах классификации и регрессии. 
	
	\textcolor{red}{нужны ли примеры?}
	Например, с его помощью можно решать следующие задачи:
	\begin{enumerate}
		\item ???
		\item ???
	\end{enumerate}
	
	
	Работа состоит из пяти разделов, заключения и списка литературы. Содержание излоежено на второй странице. Список литературы включает ??? наименований.
	
	Во \textbf{Введении} обосновываются цели и задачи исследования, его научная и практическая значимость.
	
	В \textbf{Разделе 2} вводятся основные определения и ставится формулируется постановка задачи
	
	В \textbf{Разделе 3} проводится анализ существующих методов решения задачи поиска архитектуры нейросетевой модели, а также повышения робастности таких методов относительно состязательных атак.
	
	В \textbf{Разделе 4} описывается теоретическое обоснование предлагаемого метода.
	
	В \textbf{Разделе 5} описываются используемые данные, параметры обучения, вычислительный эксперимент и анализ полученных результатов.
	
	В \textbf{Заключении} фиксируются основные результаты работы и указываются направления дальнейших исследований.
	
	
	
	\newpage
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Постановка задачи}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\subsection{Основные понятия и определения}
	Перед тем, как формально описать постановку задачи, введём несколько определений.
	
	определение модели
	
	определение параметров структуры
	
	определение структуры
	
	
	\subsection{Формальная постановка задачи}
	
	Задан набор данных $\mathfrak{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^{N}$\, где каждому входу $\mathbf{x}_i\in \mathbf{X}$ соответствует целевая переменная $y_i\in \mathbf{Y}$. Элементы $(\mathbf{x}_i, y_i)$ являются случайными величинами, взятыми из совместного распределения $\mathbf{p}(\mathbf{x}, y)$. Назовём через $\Gamma$ - суперграф архитектуры (как в DARTS), $\mathcal{A}\subset \Gamma$ - архитектура,  $\mathbf{w_{\mathcal{A}}} \sim p(\mathbf{w_{\mathcal{A}}})$ - её структурные параметры. Через $\mathbf{w} \sim p(\mathbf{w}|\Gamma, h)$ обозначим параметры модели. 
	
	Вероятностная модель задается следующим образом
	$$p(\mathbf{w}, \mathbf{\Gamma}|\mathbf{X}, \mathbf{y}, \theta) = p(\mathbf{w}|\mathbf{\Gamma}, \mathbf{X}, \mathbf{y}, \theta)\cdot p(\mathbf{\Gamma}|\mathbf{X}, \mathbf{y}, \theta).$$
	
	В качестве оптимальных параметров $\mathbf{w}^{*}, \mathbf{w^{*}_{\mathcal{A}}}$ предлагается использовать те, которые максимизируют их совместное условное распределение.
	
	Таким образом ставится следующая оптимизационная задача:
	$$\mathbf{w}^{*}, \mathbf{w^{*}_{\mathcal{A}}} = \argmax p(\mathbf{w}, \mathbf{w_{\mathcal{A}}}|\mathbf{X}, \mathbf{y}, \theta^{*}),$$
	
	$$p(\theta|\mathbf{X}, \mathbf{y})\propto p(\mathbf{y}|\mathbf{X}, \theta)\cdot p(\theta).$$
	
	
	\newpage
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Обзор существующих методов}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	В данной работе рассматривается задача поиска архитектуры модели глубокого обучения с использованием байесовского выбора \cite{NIPS2011_7eb3c8be}. Под моделью понимается суперпозиция дифференцируемых функций, решающая задачу классификации или регрессии. Под поиском архитектуры модели понимается поиск оптимальных структурных параметров. 
	
	Рассматриваемая задача имеет несколько подходов к решению. В работах \cite{DBLP:journals/corr/abs-1802-03268, DBLP:journals/corr/ZophL16} поиск архитектуры ставится как задача обучения с подкреплением, где роль агента выполняет LSTM \cite{article}, а наградой - качество сгенерированной архитектуры. Работа \cite{real2019regularized} предлагает использовать генетические алгоритмы. В \cite{DBLP:journals/corr/abs-1806-09055} путем релаксации дискретного множества операций из пространства поиска в непрерывное задача решается с помощью градиентных методов. Также к рассматриваемой задаче применяется байесовский подход оценки распределения параметров и структуры архитектуры с применением скрытых марковских цепей \cite{DBLP:journals/corr/abs-2007-16149}, случайных процессов \cite{inproceedings, DBLP:journals/corr/abs-2011-06006} вариационного вывода \cite{ferianc2021vinnas, zhou2019bayesnas}.
	
	В настоящей работе предлагается провести байесовский вывод апостериорного совместного распределения параметров и структуры модели в предположении о их зависимости. В вычислительном эксперименте предлагается найти оптимальные архитектуры на наборах данных CIFAR-10 \cite{cifar10} и FashionMNIST \cite{DBLP:journals/corr/abs-1708-07747}.
	
	\subsection{Невероятностные методы}
	Рассказать про RL \cite{Zoph2017}, генетику, градиентные подходы \cite{Liu2018}.
	
	\subsection{Вероятностные методы}
	Рассказать про VINNAS \cite{Ferianc2020}, BayesNAS \cite{Zhou2019}, DrNAS \cite{Chen2020}, BANANAS \cite{White2019}
	
	
	
	\newpage
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Описание метода}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\subsection{Вариационная нижняя оценка обоснованности}
	
	Т.к. вычисление обоснованности $p(\mathbf{y}|\mathbf{X, h})$ является очень сложной задачей, предлагается использовать метод вариационной нижней оценки обоснованности. Для этого перепишем логарифм обоснованности следующим образом
	
	$$\log p(\mathbf{y}| \mathbf{X, h}) = \iint_{\mathbf{W, \Gamma}}q(\mathbf{w, \Gamma}| \mathbf{\theta})\log p(\mathbf{y}| \mathbf{X, h})d\mathbf{\Gamma}d\mathbf{w},$$
	
	где $q(\mathbf{w, \Gamma}| \mathbf{\theta})$ - вариационное совместное распределение параметров и структуры с параметризацией $\theta$.
	
	Преобразуем полученный интеграл:
	
	$$\iint_{\mathbf{W, \Gamma}}q(\mathbf{w, \Gamma}| \mathbf{\theta})\log p(\mathbf{y}| \mathbf{X, h})d\mathbf{\Gamma}d\mathbf{w} = \iint_{\mathbf{w, \Gamma}}q(\mathbf{w, \Gamma}| \mathbf{\theta})\log\frac{p(\mathbf{y, w, \Gamma}| \mathbf{X, h})}{p(\mathbf{w, \Gamma}| \mathbf{X, y, h})}d\mathbf{\Gamma}d\mathbf{w} = $$
	
	$$=\iint_{\mathbf{w, \Gamma}}q(\mathbf{w, \Gamma}| \mathbf{\theta})\log\frac{p(\mathbf{y, w, \Gamma})q(\mathbf{w, \Gamma}|\theta)}{p(\mathbf{w, \Gamma}| \mathbf{X, y, h})q(\mathbf{w, \Gamma}|\theta)}d\mathbf{\Gamma}d\mathbf{w}=$$
	
	$$= \iint_{\mathbf{w, \Gamma}}q(\mathbf{w, \Gamma}| \mathbf{\theta})\log\frac{p(\mathbf{y, w, \Gamma}| \mathbf{X, h})}{q(\mathbf{w, \Gamma}| \mathbf{\theta})}d\mathbf{\Gamma}d\mathbf{w} + \iint_{\mathbf{w, \Gamma}}q(\mathbf{w, \Gamma}| \mathbf{\theta})\log\frac{q(\mathbf{w, \Gamma}| \mathbf{\theta})}{p(\mathbf{w, \Gamma}| \mathbf{X, y, h})}d\mathbf{\Gamma}d\mathbf{w}=$$
	
	$$=\Expect_{q(\mathbf{w, \Gamma}| \mathbf{\theta})}\Bigl[\log\frac{p(\mathbf{y, w, \Gamma}| \mathbf{X, h})}{q(\mathbf{w, \Gamma}| \mathbf{\theta})}\Bigr] + \mathcal{D}_{KL}\bigl(q(\mathbf{w, \Gamma}| \mathbf{\theta}) \ ||\  p(\mathbf{w, \Gamma}| \mathbf{X, y, h})\bigr)$$.
	
	В правом слагаемом присутствует апостериорное распределение параметров и структуры, которое требуется оценить. Заметим, что минимизации дивергенции Кульбака-Лейблера эквивалентна максимизация первого слагаемого. Распишем его более подробно:
	
	$$\Expect_{q(\mathbf{w, \Gamma}| \mathbf{\theta})}\Bigl[\log\frac{p(\mathbf{y, w, \Gamma}| \mathbf{X, h})}{q(\mathbf{w, \Gamma}| \mathbf{\theta})}\Bigr] = \Expect_{q(\mathbf{w, \Gamma}| \mathbf{\theta})}\Bigl[\log p(\mathbf{y, w, \Gamma}| \mathbf{X, h}) - \log q(\mathbf{w, \Gamma}|\theta) \Bigr]=$$
	
	$$=\Expect_{q(\mathbf{w, \Gamma}| \mathbf{\theta})}\Bigl[\log p(\mathbf{y}| \mathbf{w, \Gamma, X}) + \log p(\mathbf{w, \Gamma}|\mathbf{X, h}) - \log q(\mathbf{w, \Gamma}|\theta) \Bigr]=$$
	
	$$\Expect_{q(\mathbf{w, \Gamma}| \mathbf{\theta})}\Bigl[\log p(\mathbf{y}| \mathbf{w, \Gamma, X})  - \log \frac{q(\mathbf{w, \Gamma}|\theta)}{p(\mathbf{w, \Gamma}|\mathbf{X, h})} \Bigr]=$$
	
	$$\Expect_{q(\mathbf{w, \Gamma}| \mathbf{\theta})}\Bigl[\log p(\mathbf{y}| \mathbf{w, \Gamma, X})\Bigr]  - \mathcal{D}_{KL}\bigl(q(\mathbf{w, \Gamma} | \theta) \ || \ p(\mathbf{w, \Gamma} | \mathbf{X, h}) \bigr).$$
	
	Теперь распишем второе слагаемое полученного выражения:
	
	$$\mathcal{D}_{KL}\bigl(q(\mathbf{w, \Gamma} | \theta) \ || \ p(\mathbf{w, \Gamma} | \mathbf{X, h}) \bigr) = \mathcal{D}_{KL}\bigl(q(\mathbf{w} | \Gamma, \theta_{\mathbf{w}}) \cdot q(\mathbf{\Gamma} |\theta_{\mathbf{\Gamma}}) \ || \ p(\mathbf{w} | \mathbf{\Gamma, X, h}) \cdot p(\mathbf{\Gamma} | \mathbf{X, h}) \bigr)=$$
	
	$$=\mathcal{D}_{KL}\bigl(q_{\mathbf{\Gamma}}(\mathbf{\Gamma}|\theta_{\mathbf{\Gamma}}) \ || \ p(\mathbf{\Gamma} | \mathbf{h}) \bigr) + \Expect_{q_{\mathbf{w}}(\mathbf{w} | \mathbf{\Gamma, \theta_{\mathbf{w}}})} \Expect_{q_{\mathbf{\Gamma}}(\mathbf{\Gamma} | \mathbf{\theta_{\mathbf{\Gamma}}})} \log\\frac{q_{\mathbf{w}}(\mathbf{w} | \mathbf{\Gamma, \theta_{\mathbf{w}}})}{p(\mathbf{w}|\mathbf{\Gamma, h})}.$$
	
	Таким образом, оптимизируемый функционал качества имеет следующий вид:
	
	$$\mathcal{L} = \Expect_{q(\mathbf{w, \Gamma}| \mathbf{\theta})}\Bigl[\log p(\mathbf{y}| \mathbf{w, \Gamma, X})\Bigr] - \mathcal{D}_{KL}\bigl(q_{\mathbf{\Gamma}}(\mathbf{\Gamma}|\theta_{\mathbf{\Gamma}}) \ || \ p(\mathbf{\Gamma} | \mathbf{h}) \bigr) + \Expect_{q_{\mathbf{w}}(\mathbf{w} | \mathbf{\Gamma, \theta_{\mathbf{w}}})} \Expect_{q_{\mathbf{\Gamma}}(\mathbf{\Gamma} | \mathbf{\theta_{\mathbf{\Gamma}}})} \log\frac{q_{\mathbf{w}}(\mathbf{w} | \mathbf{\Gamma, \theta_{\mathbf{w}}})}{p(\mathbf{w}|\mathbf{\Gamma, h})}$$.
	
	Заметим, что он состоит из матожидания правдоподобия модели, KL-дивергенции между вариационным и априорным распределением структуры и двойным матожиданием логарифма отношения вариационного и априорного распределения на параметры модели.
	
	
	\subsection{Выбор априорных распределений}
	
	
	\newpage
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Вычислительный эксперимент}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	Проводится с помощью ЯП Python и специализированных библиотек глубокого обучения pytorch и nni.
	
	Описать эксперимент, пространство поиска, параметры обучения (скорее всего метапараметры, придерживаясь нотации Олега)
	
	На выходе: 
	\begin{itemize}
		\item обязательно сравнительная таблица с другими алгоритмами (точность, число параметров, время поиска, робастность относительно adversarial атак)
		\item картинка с выученной архитектурой
	\end{itemize}
	
	
	
	\newpage
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Заключение}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\cite{Potapczynski2019}
	
	\newpage
	
	\bibliographystyle{gost71s}
	\bibliography{biblio}
	
\end{document}
