\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\newcommand{\hdir}{.}

\usepackage{graphicx}
\usepackage{multirow}



\begin{document}

%\title
%    [Байесовский выбор архитектуры нейросетевой модели] % краткое название; не нужно, если полное название влезает в~колонтитул
%    {Байесовский выбор архитектуры нейросетевой модели}
%\author
%    [А.\,Д.~Сотников,  О.\,Ю.~Бахтеев] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
%    {А.\,Д.~Сотников,  О.\,Ю.~Бахтеев} % основной список авторов, выводимый в оглавление
%    [А.\,Д.~Сотников,  О.\,Ю.~Бахтеев] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
%\email
%    { sotnikov.ad@phystech.edu; bakhteev@phystech.edu}

%\begin{abstract}
    {\textcolor{black}{В работе исследуется задача выбора структуры модели нейронной сети. Вводятся априорные распределения на параметры, гиперпараметры и структуру модели. Предполагается зависимость параметров от структуры модели.  Предлагается метод, вычисляющий апостериорного совместного распределения структуры и параметров модели с помощью байесовского вывода. В силу практической невычислимости такого вывода распределение предлагается оценивать с помощью оптимизации вариационной нижней оценки.}
%\end{abstract}
    

\bigskip
\noindent
\textbf{Ключевые слова}: \emph {выбора структуры модели нейронной сети; нейронные сети; вариационный вывод;.}
}

%\maketitle
%\linenumbers

\section{Введение}
В данной работе рассматривается задача поиска архитектуры модели глубокого обучения с использованием байесовского выбора \cite{NIPS2011_7eb3c8be}. Под моделью понимается суперпозиция дифференцируемых функций, решающая задачу классификации или регрессии. Под поиском архитектуры модели понимается поиск оптимальных структурных параметров. В качестве базового алгоритма используется дифференцируемый алгоритм поиска архитектуры DARTS \cite{DBLP:journals/corr/abs-1806-09055}, основанный на использовании  релаксации ~--~ перевода множества допустимых структурных параметров из дискретного в непрерывное. В работе предлагается использовать градиентные методы оптимизации в силу более эффективного использования ими вычислительных ресурсов в сравнении с методами, работающими на дискретном множестве структурных параметров. Предлагаемый в работе метод основывается на байесовском выводе архитектуры нейросетевой модели. Подход заключается в применении вариационного вывода \cite{NIPS2011_7eb3c8be} для обучения распределений параметров структуры, используемых в \cite{DBLP:journals/corr/abs-1806-09055}. работает со свёрточными и рекуррентными нейронными сетями.

\textcolor{red}{нужно шде-то кратко рассказать про DARTS и используемый в нем гиперграф наверное}

Существуют несколько методов, осуществляющих поиск архитектуры с использованием байесовского выбора. В работе \cite{zhou2019bayesnas} формулируется проблема для \textcolor{red}{zero-shot NAS}, заключающаяся в неправильном обращении с \textcolor{red}{zero operations} на этапе выявления наиболее важных связей между узлами архитектурами. В качестве решения этой проблемы предлагается использовать байесовский вывод с \textcolor{red}{a hierarchical automatic
relevance determination prior} для структурных параметров. Такой подход исключает возможность переобучения и позволяет не настраивать большинство гиперпараметров модели.

Метод, описанный в \cite{DBLP:journals/corr/abs-1910-11858}, применяет байесовскую оптимизацию для поиска структурных параметров \textcolor{red}{дописать, пока не разобрался}


Помимо вышеописанных методов существуют работы, основанные на применении гауссовских процессов \cite{inproceedings, DBLP:journals/corr/abs-2011-06006} и скрытых марковских цепей \cite{DBLP:journals/corr/abs-2007-16149}. \textcolor{red}{тут расписать подробнее эти методы}
Существуют подходы на основе применения байесовских нейронных сетей с использованием гауссовских процессов \cite{inproceedings, DBLP:journals/corr/abs-2011-06006}

В работе \cite{DBLP:journals/corr/abs-2006-10355} описывается недостаток метода DARTS, заключающийся в том, что прямое обучение структурных параметров, отвечающих за веса в mixed operation, приводит к переобучению и росту ошибки на валидационном наборе данных. Для устранения этого недостатка предлагается задавать априорное распределение структурных параметров распределением Дирихле.

Метод, описанный в \cite{ferianc2021vinnas}, использует вариационный вывод распределений структурных параметров и параметров (весов) модели. Такой подход имеет ряд преимуществ: решается проблема \textcolor{red}{mode collapse}, связанная с выбором одного типа операции в процессе поиска структуры модели; проблема локальных минимумов на различных датасетах и множествах поиска. С другой стороны метод имеет ряд ограничений ~- \textcolor{red}{какие?}

В настоящей работе предлагается отойти от предположения о независимости распределения параметров модели и структурных параметров. Кроме того, в качестве априорного распределения на структурные параметры предлагается взять распределение Дирихле. \textcolor{red}{структурировать и дописать конкретику}.


\bigskip
\textcolor{red}{/////////////////////////// абзацы про научную новизну, теор. значимость и практ. значимость скорее для диплома, нежели статьи, но для понимания и фиксирования происходящего закреплю его пока здесь}

\textbf{Научная новизна:}

\textbf{Теоретическая значимость.} В работе исследуется вариационный вывод  распределения параметров модели в предположении его зависимости от распределения структуры модели. Кроме того, в качестве априорного распределения на параметры модели используется распределение Дирихле??, что является "наиболее правильным и общим" случаем рассматриваемой задачи.

\textbf{Практическая значимость.} Предложенный в работе метод предназначен для решения задач классификаии и регрессии, автоматического поиска наилучшей (но в каком смысле?) архитектуры модели глубокого обучения на заданном наборе данных.

\textcolor{red}{///////////////////////////}


\section{Постановка задачи}

Задан набор данных $\mathfrak{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^{N}$\, где каждому входному вектору $\mathbf{x}_i\in \mathbf{X}$ соответствует целевая переменная $y_i\in \mathbf{Y}$. Элементы $(\mathbf{x}_i, y_i)$ являются случайными величинами, взятыми из совместного распределения $\mathbf{p}(\mathbf{x}, y)$. Через $\Gamma$ назовем параметризованное семейство структурных параметров архитектуры. Оно также представимо в виде ориентированного ациклического графа (рис. 1). \textcolor{red}{вставить картинку из DARTS}. Каждое ребро $(j, k)$ графа представляет собой набор $K$ операций с весами, заданными через структурные параметры $\mathbf{\gamma}^{j, k}\in [0, 1]^{K^{j, k}}$. Архитектурой $\mathcal{A}\in \Gamma$ назовем граф, в котором каждому ребру соответствует одна операция. 

Обозначим через $\mathbf{w} \sim p(\mathbf{w})$ параметры модели.

Вероятностная модель задается следующим образом
$$p(\mathbf{w}, \mathbf{w_{\mathcal{A}}}|\mathbf{X}, \mathbf{y}, \theta) = p(\mathbf{w}|\mathbf{w_{\mathcal{A}}}, \mathbf{X}, \mathbf{y}, \theta)\cdot p(\mathbf{w_{\mathcal{A}}}|\mathbf{X}, \mathbf{y}, \theta).$$

В качестве оптимальных параметров $\mathbf{w}^{*}, \mathbf{w^{*}_{\mathcal{A}}}$ предлагается использовать те, которые максимизируют их совместное условное распределение.

Таким образом ставится следующая оптимизационная задача:
$$\mathbf{w}^{*}, \mathbf{w^{*}_{\mathcal{A}}} = \argmax p(\mathbf{w}, \mathbf{w_{\mathcal{A}}}|\mathbf{X}, \mathbf{y}, \theta^{*}),$$
$$p(\theta|\mathbf{X}, \mathbf{y})\propto p(\mathbf{y}|\mathbf{X}, \theta)\cdot p(\theta).$$

Т.к. вычисление обоснованности $p(\mathbf{y}|\mathbf{X, h})$ является очень сложной задачей, предлагается использовать метод вариационной нижней оценки обоснованности. Для этого перепишем логарифм обоснованности следующим образом

$$p(\mathbf{y}|\mathbf{X, h}) = \iint_{\mathbf{w, \Gamma}} p(\mathbf{y}|\mathbf{X, w, \Gamma})p(\mathbf{w}|\mathbf{\Gamma, h})p(\mathbf{\Gamma}|\mathbf{h})d\mathbf{\Gamma}d\mathbf{w}$$

\section{Существующие подходы}
\subsection{Подходы, использующие обучение с подкреплением}

\subsection{Градиентные методы}

\subsection{Вероятностные подходы}

\section{Предлагаемый метод}

\section{Вычислительный эксперимент}


\section{Заключение}
В работе рассматривалась задача поиска архитектуры нейросетевой модели. Исследовался метод поиска структурных параметров модели, основанный на вариационном выводе, в предположении зависимости распределения параметров модели от структурных параметров архитектуры. В ходе экспериментов были получены следующие результаты \textcolor{red}{после экспериментов дописать}


\newpage
\bibliography{biblio.bib}
\bibliographystyle{jmlda-rus.bst}
%\nocite{*}

\end{document}


